#%% [markdown]
# ## Start 
# ## 1. Pobranie danych i sprawdzenie zawartości zbioru
# ## 2. Podział na zbiór trenujący i testowy
# ## 3. Wizualizacja i odkrywanie danych
# ## 4. Przygotowanie danych pod algorytmy uczenia maszynowego
# ## 5. Wybór algorytmu uczenia maszynowego i uczenie modelu
# ## 6. Odczytanie zapisanych modeli i wizualizacja wyników.

#%% [markdown]
# ## 1. Pobranie danych i sprawdzenie zawartości zbioru.
# Wczytywanie danych z bazy danych SQLite.
# Dane pochodzą z portalu Otomoto.pl odczytane przy pomocy web scrapera.
# Scraper został uruchomiony 14 lipca 2019 roku i zapisał do bazy danych,
# ponad 140 tesięcy ogłoszeń.
# Dane zawierają cechy samochodów wystawionych na sprzedaż wraz z ceną.
#%%

import pandas as pd
import numpy as np
import os
import sklearn
import sqlite3
from sqlite3 import Error 

DATASET_PATH = os.path.join("dataset", "data-car")
DBPATH = 'dataset\otomoto_v2.db'

def creat_connect_bd(db_path):
    try:
        conn = sqlite3.connect(db_path)
    except Exception as e:
        print(e)
    return conn

def closeConnection(conn):
    try:
        conn.close()
    except Error as e:
        print(e)


def select_car_data(bd_path):
    conn = creat_connect_bd(bd_path)
    try:
        DF = pd.read_sql_query('SELECT * FROM cars_tab', conn)
        closeConnection(conn)
    except Exception as e:
        print(e)
    return DF

#%%
# Pobranie danych
cars = select_car_data(DBPATH)

#%% [markdown]
# Zamian brakujących wartośći z Null na NaN.
# Pierwsze spojrzenie na dataset. Wyświetlenie informacji o wielkości, opisie danych.
cars.fillna(value=pd.np.nan, inplace=True)
cars.info()
cars.dtypes


#%% [markdown]
# Sprawdzenie jakie obiekty są dostępne w kolumnach z danymi katygorialnymi
# Marka pojazdu
cars["marka"].value_counts()

#%%[markdown]
# Model pojazdu
cars["model"].value_counts()

#%%[markdown]
#Typ paliwa
#
cars["rodzaj_paliwa"].value_counts()

#%% [markdown]
# Rodzaj skrzyni biegów
#
cars["skrzynia_biegów"].value_counts()

#%% [markdown]
# Rodzaj napędu
#
cars["naped"].value_counts()

#%% [markdown]
# Sprawdzenie jakie obiekty są dostępne w kolumnach z danymi numerycznymi
# Rok produkcji
cars["rok_produkcji"].value_counts()

#%% [markdown]
# Liczba drzwi
cars["liczba_drzwi"].value_counts()

#%% [markdown]
# Liczba miejsc
cars["liczba_miejsc"].value_counts()

#%% [markdown]
# Opis danych numerycznych poprzez wyświetlenie wartości min, max i odchylenie standardowe
cars.describe()

#%% [markdown]
# Wyświetlenie wykresów histogramu aby jepiej się przyjzeć danym. 
# Wykresy dla danych numerycznych

import matplotlib.pyplot as plt
cars.hist(bins=50, figsize=(20,15))

#%% [markdown]
# ## 2. Usunięcie danych odstających oraz podział na zbiór trenujący i testowy

# Usówaniwe kolumny opis, ponieważ dane w niej zawarte nie są ustrukturyuzowane, nie posiadają powetarzalności
# Lemetyzacj, tokenizacja, stiming, bag of words ...

cars = cars.drop("opis", axis=1)
cars.shape


#%%
# Usówanie danych które nie sa wiarygodne jak na przykład auto ze 120 000 drzwiami.
# Odrzucanie ogłoszeń gdzie liczba drzwi jest większa niż 5
cars = cars[cars["liczba_drzwi"] != 6]
cars = cars[cars["liczba_drzwi"] != 7]
cars = cars[cars["liczba_drzwi"] != 8]
cars = cars[cars["liczba_drzwi"] != 9]
cars = cars[cars["liczba_drzwi"] != 55]
cars = cars[cars["liczba_drzwi"] != 120000]

cars["liczba_drzwi"].value_counts()
cars.shape

#%%
# Odrzucenie ogłoszeń których cena przekracza milion złotych. 

cars = cars[cars["cena"] <= 1000000]
cars.shape
#%%
# Odrzucenie ogłoszeń których wiek auta jest większy niż 50 lat.
# jest inna 

cars = cars[cars["rok_produkcji"] >= 1969]
cars.shape

#%%
# Zmniejszenie efektu danych odstających poprzez wprowadzenie logarytmu

cars["pojemnosc_skokowa"] = [np.log(x) for x in cars["pojemnosc_skokowa"]]
cars["przebieg"] = [np.log(x) for x in cars["przebieg"]]
cars["moc"] = [np.log(x) for x in cars["moc"]]

#%% 
# Dodanie kolumny wiek auta
wiek = 2019-cars["rok_produkcji"]
cars.insert(27, "wiek", wiek, True)

#%% [markdown]
# Opis danych numerycznych poprzez wyświetlenie wartości min, max i odchylenie standardowe
cars.describe()
#%%
# Wyświetlenie wykresów histogramu po czyszczeniu danych. 

import matplotlib.pyplot as plt
cars.hist(bins=50, figsize=(20,15))
#%%
# Obsługa kolumny wyposażenie w pipeline nie udała się wieć wycigam jedną kolumne 
# a następnie wstawię do DF w formie one hot
from sklearn.feature_extraction.text import CountVectorizer

cars['wyposazenie'].fillna("brak wyposazenia", inplace=True)
cars_wypos = cars["wyposazenie"].copy()
cars = cars.drop("wyposazenie", axis=1)

vect = CountVectorizer()
wypos_one_hot = vect.fit_transform(cars_wypos)

wypos_one_hot.toarray()

#%%
cars.insert(27, "wyposazenie", wypos_one_hot, True)
cars.head()


#%% [markdown] Podział na zbiory
# Podziła na dane trenujące i testujące

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(cars, test_size=0.2, random_state=42)

print("Dane trenujące", len(train_set), "dane testowe", len(test_set))

#%%[markdown]
# ## 3. Wizualizacja i odkrywanie danych.
# W tym punkcie zagłąbimy się w dane jakie mamy. Sprawdzimy jakie
# występują korelacja pomiedzy danymi.

#%% Mediana cena samochodu per marka
%pylab inline
Make=train_set.groupby(['marka'])['cena'].median()
Make.plot(kind="bar", stacked=True, figsize=(12,6))
pylab.ylabel('Mediana ceny')
pylab.title('Mediana cen per marka')
show()

#%% Mediana cena per rok
%pylab inline
median_price = train_set.groupby(['rok_produkcji'])['cena'].median()
median_price.plot(kind="bar", stacked=True, figsize=(12,6))
pylab.ylabel('Mediana ceny')
pylab.title('Mediana cen per rok')
show()

#%% Marki samochodowe. Proporcje ogłoszeń
counts = train_set['marka'].value_counts()*100/sum(train_set['marka'].value_counts())
populat_make = counts.index

plt.figure(figsize=(10,15))
plt.barh(populat_make, width=counts)
plt.title('Proporcja marek samochodów w ogłoszeniach')
plt.xlabel('Proporcje')
plt.ylabel('Marki samochodowe')
plt.show()


#%% Korelacja
# Obliczamy współczynik korelacji liniowej (zwany korelacją Parsona)
import seaborn as sns
corr_matrix = train_set.corr()

#%% Sprawdzenie stopnia koleracji atrybutów do ceny samochodów
corr_matrix["cena"].sort_values(ascending=False)

#%% Wykres korelacji
mask = np.zeros_like(corr_matrix, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

f, ax = plt.subplots(figsize=(11, 9))

cmap = sns.diverging_palette(220, 10, as_cmap=True)

sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()

corr_matrix

#%%
# ## 4. Przygotowanie danych pod algorytmy uczenie maszynowego

#%%Dane uczące i wynik
Xcars = train_set.drop("cena", axis=1)
Ycars_labels = train_set["cena"].copy()

print("Średnia zbioru treningowego: ")
Ycars_labels.describe()

#%%
# Brakujące dane są w kategoriach numerycznych i kategorialnych.
# Dane numeryczne uzupełnię poprzez medianę, a dane kategorialne jako Unknow


#%% Potok transformujący dla danych numerycznych

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

cars_num = ["rok_produkcji", "wiek", 'przebieg', 'pojemnosc_skokowa', 'moc', 'liczba_drzwi', 'liczba_miejsc']

cars_num

num_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy="median")),
    ('std_scaler', StandardScaler())
])

#%% Potok dla danych kategorialnych

from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer
from sklearn.compose import ColumnTransformer
cars_cat = ['marka', 'model', 'wersja', 'uszkodzony', 'oferta_od', 'kategoria', 'rodzaj_paliwa', 'skrzynia_biegów', 'naped', 'typ', 'kolor', 'metalik', 'perlowy', 'vat_marza', 'mozliwosc_finansowania', 'kraj_pochodzenia', 'pierwszy_wlasciciel', 'bezwypadkowy', 'serwisowany_aso', 'stan']

cat_pipeline = Pipeline (steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknow')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

#%% Połączenie kolumn numerycznych i kategorialnych

preprocessor = ColumnTransformer(transformers=[
        ('num', num_pipeline, cars_num),
        ('cat', cat_pipeline, cars_cat)
])

#%% [markdown]
# ## 5. Wybór algorytmu uczenia maszynowego i uczenie modelu
# 5.1 Funkcja generująca zbiory dla hiperparametrów.
# 5.2 Funkcja wykomująca algorytm.
# 5.3 Algorytmy uczenia maszynowego
# 5.3 Zapisywanie modelu.
#%% ## Dane testowe - przygotowanie danych testowych odbędzie się podczas 
# przetwarzania pipelinie w momęcie wywołania predykcji wykonywane są też 
# kroki wstępnego przetważania odpowiadające za czyszczenie danych. 
Xcars_test = test_set.drop("cena", axis=1)
Ycars_test_labels = test_set["cena"].copy()

print("Średnia cena zbioru testowego: ")
Ycars_test_labels.describe()

#%% Zakres hiperparametrów ( 2^(-5) do 2^(10))
def range_parameters(a=2, low_range=-5, high_range=10, returnType=2):
    flag = True
    hiperparameter = []
    while flag:
        value = pow(a, low_range)
        if returnType == 1:
            hiperparameter.append(int(value))
        elif returnType == 2:
            hiperparameter.append(float(value))
        low_range += 1
        if low_range > high_range:
            flag = False
    
    return hiperparameter

zakres_parametrów = range_parameters()
# Wartości int
zakres_parametrów_int = range_parameters(a=2, low_range=-1, high_range=10, returnType=1)
print('Zakres parametrów', *zakres_parametrów, "int", zakres_parametrów_int)

min_weight_fraction_leaf = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
zakres_parametrow_minus6 = range_parameters(2, -6, 0, 2)
print('zakres -6 0', zakres_parametrow_minus6)
zakres_parametrów_jeden_plus = range_parameters(2, 0, 10, returnType=1)
print('Zakres parametrów jeden plus', zakres_parametrów_jeden_plus)

#%% Biblioteki w punkcie 5
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge
from sklearn.externals import joblib
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBClassifier, XGBRegressor
from sklearn.neural_network import MLPRegressor
from math import sqrt

# Stałe gridSearch
# Ograniczenie 
job = -1

# Trenowanie, odczytywanie i zapisywanie modelu 
def model_execut(gridSearch, name, scoring, name_file):
    # Trenowanie modelu
    print(name)
    gridSearch.fit(Xcars, Ycars_labels)
    # Najlepsze parametry
    print('Najlepsze parametry: %s' % gridSearch.best_params_)
    # Najlepszy wynik
    print('Najlepszy wynik dane trenujące RMSE: %.3f' % sqrt(-gridSearch.best_score_))
    # Predykcja na danych testowych z najlepszymi znalezionymi parametrami
    y_pred = gridSearch.predict(Xcars_test)
    rmse = sqrt(scoring(Ycars_test_labels, y_pred))
    # Sprawdzenie modelu z najlepszymi parametrami
    print('Wynik na danych testowych dla najlepszych parametrów RMSE: %.3f' %rmse )
    # Zapisanie najlepaszego modelu
    joblib.dump(gridSearch, name_file)
    #Lista z zapisanymi modelami
    file_model_list = []
    file_model_list.append(name_file)



#%% [markdown]
#  ##Regresja liniowa

pipe_LinearRegression = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('linReg', LinearRegression())
])

gridParams_LinearRegression = [{'linReg__normalize': [True, False],
                                'linReg__fit_intercept': [True, False],
                                'linReg__copy_X': [True, False]}]

gs_LinearRegression = GridSearchCV(estimator=pipe_LinearRegression,
                                    param_grid=gridParams_LinearRegression,
                                    scoring='neg_mean_squared_error',
                                    cv=10)

# Trenowanie modelu
name_file = 'LinearRegression.pkl'
model_execut(gs_LinearRegression, LinearRegression, mean_squared_error, name_file)

#%% [markdown] 
# ## Regresja Lasso
pipe_Lasso = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('lasso', Lasso())
])

gridParams_Lasso = [{'lasso__alpha': zakres_parametrów, 
                     'lasso__fit_intercept': [True, False],
                     'lasso__normalize': [False, True],
                     'lasso__copy_X': [True, False],
                     'lasso__max_iter': zakres_parametrów_int,
                     'lasso__warm_start': [True, False],
                     'lasso__selection': ['cyclic', 'random']}]

gs_Lasso = GridSearchCV(estimator=pipe_Lasso,
                                    param_grid=gridParams_Lasso,
                                    scoring='neg_mean_squared_error',
                                    cv=10,
                                    n_jobs=job)

# Trenowanie modelu
name_file = 'Lasso.pkl'
model_execut(gs_Lasso, Lasso, mean_squared_error, name_file)

#%% [markdown]
# ## Regresja Ridge
pipe_Ridge = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('Ridge', Ridge())
])

gridParams_Ridge = [{'Ridge__alpha': zakres_parametrów,
                     'Ridge__fit_intercept': [True, False],
                     'Ridge__copy_X': [True, False],
                     'Ridge__solver': ['auto', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}]

gs_Ridge = GridSearchCV(estimator=pipe_Ridge,
                                    param_grid=gridParams_Ridge,
                                    scoring='neg_mean_squared_error',
                                    cv=10,
                                    n_jobs=job)

# Trenowanie modelu
name_file = 'Ridge.pkl'
model_execut(gs_Ridge, Ridge, mean_squared_error, name_file)

#%% [markdown]
# ## Drzewa decyzyjne regresja

pipe_DTR = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('DTR', DecisionTreeRegressor())
])

gridParams_DTR = [{'DTR__splitter': ['best', 'random'],
                     'DTR__max_depth': zakres_parametrów_jeden_plus,
                     'DTR__min_samples_leaf': zakres_parametrów_jeden_plus,
                     'DTR__min_weight_fraction_leaf': min_weight_fraction_leaf,
                     'DTR__min_impurity_decrease': zakres_parametrów},
                     {'DTR__splitter': ['best', 'random'],
                     'DTR__max_depth': zakres_parametrów_jeden_plus,
                     'DTR__min_samples_leaf': zakres_parametrów_jeden_plus,
                     'DTR__min_weight_fraction_leaf': min_weight_fraction_leaf,
                     'DTR__min_impurity_decrease': zakres_parametrów}]

gs_DTR = GridSearchCV(estimator=pipe_DTR,
                                    param_grid=gridParams_DTR,
                                    scoring='neg_mean_squared_error',
                                    cv=10,
                                    n_jobs=job)

# Trenowanie modelu
name_file = 'DecisionTreeRegressor.pkl'
model_execut(gs_DTR, DecisionTreeRegressor, mean_squared_error, name_file)

#%% [markdown]
# ## Las losowy
pipe_Forest = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('Forest', RandomForestRegressor())
])

gridParams_Forest = [{'Forest__n_estimators': zakres_parametrów_jeden_plus,
                     'Forest__min_samples_leaf': zakres_parametrów_jeden_plus,
                     'Forest__bootstrap': [True],
                     'Forest__oob_score': [True, False]},]

gs_Forest = GridSearchCV(estimator=pipe_Forest,
                                    param_grid=gridParams_Forest,
                                    scoring='neg_mean_squared_error',
                                    cv=10,
                                    n_jobs=job)

# Trenowanie modelu
name_file = 'RandomForestRegressor.pkl'
model_execut(gs_Forest, RandomForestRegressor, mean_squared_error, name_file)

#%% [markdown]
# ## Boosting XGBoost

pipe_XGBoost = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('xgbre', XGBRegressor())
])

gridParams_xgbre = [{'xgbre__booster': ['gblinear'],
                     'xgbre__max_depth': [1, 2, 3],
                     'xgbre__learning_rate': [0.05, 0.1, 0.3],
                     'xgbre__n_estimators': [500, 1000, 2000],
                     'xgbre__reg_alpha': [0, 0.5, 1],
                     'xgbre__reg_lambda': [0, 0.1, 0.2]},]

gs_xgbre = GridSearchCV(estimator=pipe_XGBoost,
                                    param_grid=gridParams_xgbre,
                                    scoring='neg_mean_squared_error',
                                    cv=5,
                                    n_jobs=job)

# Trenowanie modelu
name_file = 'XGBoostReg.pkl'
model_execut(gs_xgbre, XGBRegressor, mean_squared_error, name_file)

#%% [markdown]
# ## Sieci neuronowe

pipe_MLPR = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('mlpr', MLPRegressor())
])

gridParams_mlpr = [{'mlpr__hidden_layer_sizes': [2, 100],
                     'mlpr__max_iter': [200, 5000],
                     'mlpr__activation': ['identity', 'logistic', 'tanh', 'relu'],
                     'mlpr__solver': ['lbfgs', 'adam']},]


gs_mlpr = GridSearchCV(estimator=pipe_MLPR,
                                    param_grid=gridParams_mlpr,
                                    scoring='neg_mean_squared_error',
                                    cv=10,
                                    n_jobs=job)

# Trenowanie modelu
name_file = 'neural_network_regressor.pkl'
model_execut(gs_mlpr, MLPRegressor, mean_squared_error, name_file)

#%% [markdown]
# ## 6. Odczytanie zapisanych modeli i wizulizacja wyników

import matplotlib.pyplot as plt

def czytanie_modeli(gs, name, score):
    print(name)
    print('Odczytanie zapisanego modelu: ')
    print("Najlepsze parametry: ", gs.best_params_)
    print("Najlepszy estimator: ", gs.best_estimator_)
    print('Najlepszy wynik na danych treningowych MSE: ', -gs.best_score_)
    print('Najlepszy wynik na danych treningowych RMSE: ', sqrt(-gs.best_score_))
    print('Predykcja danych testowych. ')
    ypred2 = gs.predict(Xcars_test)

    print('Wynik na danych testowych MSE: ', score(Ycars_test_labels, ypred2))
    print('Wynik na danych testowych RMSE: ', sqrt(score(Ycars_test_labels, ypred2)) )

    x1 = list(Ycars_test_labels)
    x2 = list(ypred2)
    colors = ['green', 'orange']
    names = ['Dane testowe', 'Dane predykcja']  
    plt.figure(figsize=(15, 10))
    plt.hist([x1, x2], bins = 100, color = colors, label = names, density=True)
    plt.legend(prop={'size': 10})
    plt.title('Histogram predykcji')
    plt.xlabel('Cena')
    plt.ylabel('Przykłady')




#%% [markdown]
# ## 6.1. Regeresja liniowa

loaded_linear_regresion = joblib.load("LinearRegression.pkl")

czytanie_modeli(loaded_linear_regresion, 'Regresja liniowa', mean_squared_error)

#%% [markdown]
# ## 6.2. Regresja Lasso

Lasso = joblib.load("Lasso.pkl")
czytanie_modeli(Lasso, "Regresja  Lasso", mean_squared_error)

#%% [markdown]
# ## 6.3. Regresja brzegowa

Ridge = joblib.load("Ridge.pkl")
czytanie_modeli(Ridge, "Regresja brzegowa", mean_squared_error)

#%% [markdown]
# ## 6.4. Dzewa decyzyjne (regresja)

DecisionTreeRegressor = joblib.load("DecisionTreeRegressor.pkl")
czytanie_modeli(DecisionTreeRegressor, "Drzewa decyzyjne", mean_squared_error)

#%% [markdown]
# ## 6.4. Losowy las

RandomForestRegressor = joblib.load("RandomForestRegressor.pkl")
czytanie_modeli(RandomForestRegressor, "Losowy las", mean_squared_error)

#%% [markdown]
# ## 6.5. Wzmacnianie gradientowe XGBoost

XGBoostReg = joblib.load("XGBoostReg.pkl")
czytanie_modeli(XGBoostReg, "Wzmacnianie gradientowe", mean_squared_error)

#%%[markdown]
# ## 6.6. Sieci neuronowe

neural_network_regressor = joblib.load("neural_network_regressor.pkl")
czytanie_modeli(neural_network_regressor, "Sieci neuronowe", mean_squared_error)
